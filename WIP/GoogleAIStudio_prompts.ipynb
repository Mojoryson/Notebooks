{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to use Google AI Studio and demonstrate a few different prompting techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -U -q google-generativeai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rmorrison/anaconda3/envs/ai_dev/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display\n",
    "\n",
    "genai.configure(api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: models/chat-bison-001\n",
      "Model: models/text-bison-001\n",
      "Model: models/embedding-gecko-001\n",
      "Model: models/gemini-1.0-pro-latest\n",
      "Model: models/gemini-1.0-pro\n",
      "Model: models/gemini-pro\n",
      "Model: models/gemini-1.0-pro-001\n",
      "Model: models/gemini-1.0-pro-vision-latest\n",
      "Model: models/gemini-pro-vision\n",
      "Model: models/gemini-1.5-pro-latest\n",
      "Model: models/gemini-1.5-pro-001\n",
      "Model: models/gemini-1.5-pro-002\n",
      "Model: models/gemini-1.5-pro\n",
      "Model: models/gemini-1.5-pro-exp-0801\n",
      "Model: models/gemini-1.5-pro-exp-0827\n",
      "Model: models/gemini-1.5-flash-latest\n",
      "Model: models/gemini-1.5-flash-001\n",
      "Model: models/gemini-1.5-flash-001-tuning\n",
      "Model: models/gemini-1.5-flash\n",
      "Model: models/gemini-1.5-flash-exp-0827\n",
      "Model: models/gemini-1.5-flash-002\n",
      "Model: models/gemini-1.5-flash-8b\n",
      "Model: models/gemini-1.5-flash-8b-001\n",
      "Model: models/gemini-1.5-flash-8b-latest\n",
      "Model: models/gemini-1.5-flash-8b-exp-0827\n",
      "Model: models/gemini-1.5-flash-8b-exp-0924\n",
      "Model: models/gemini-2.0-flash-exp\n",
      "Model: models/gemini-exp-1206\n",
      "Model: models/gemini-exp-1121\n",
      "Model: models/gemini-exp-1114\n",
      "Model: models/learnlm-1.5-pro-experimental\n",
      "Model: models/embedding-001\n",
      "Model: models/text-embedding-004\n",
      "Model: models/aqa\n"
     ]
    }
   ],
   "source": [
    "# List available models\n",
    "for m in genai.list_models():\n",
    "    print(f\"Model: {m.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Define which Model you want to use\n",
    "2. Send a query to the model \n",
    "3. Display actual response and formatted response (using Markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"text\": \"Imagine you have a really smart puppy.  You teach the puppy tricks, like \\\"sit\\\" and \\\"fetch.\\\"  The more you teach it, the better it gets at those tricks.\\n\\nAI is like a super-smart puppy, but instead of tricks, we teach it to do things on a computer.  We show it lots and lots of pictures of cats, and then it learns to recognize cats in new pictures!  Or we show it lots of words, and it learns to write stories.\\n\\nIt's not really *thinking* like you and me, it's just really good at following instructions and learning from examples.  It's like a very clever machine that can learn to do amazing things!\\n\"\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"avg_logprobs\": -0.14458225868843697\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 9,\n",
      "        \"candidates_token_count\": 148,\n",
      "        \"total_token_count\": 157\n",
      "      }\n",
      "    }),\n",
      ") \n",
      " ---------------- \n",
      " Markdown Response \n",
      " -------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart puppy.  You teach the puppy tricks, like \"sit\" and \"fetch.\"  The more you teach it, the better it gets at those tricks.\n",
       "\n",
       "AI is like a super-smart puppy, but instead of tricks, we teach it to do things on a computer.  We show it lots and lots of pictures of cats, and then it learns to recognize cats in new pictures!  Or we show it lots of words, and it learns to write stories.\n",
       "\n",
       "It's not really *thinking* like you and me, it's just really good at following instructions and learning from examples.  It's like a very clever machine that can learn to do amazing things!\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the model for a resopnse\n",
    "\n",
    "# Use a model from the list\n",
    "flash = genai.GenerativeModel(\"gemini-1.5-flash\") \n",
    "\n",
    "# Query the model\n",
    "response = flash.generate_content(\"Explain AI to a 5 year old\") \n",
    "\n",
    "# Raw response from the model\n",
    "print(f\"{response} \\n ---------------- \\n Markdown Response \\n -------------------\") \n",
    "# Markdown fomatted response\n",
    "Markdown(response.text) # Display the response in markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start a chat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello Rod, it's nice to meet you!  How can I help you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message(\"Hello, my name is Rod\")\n",
    "print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: One of the most interesting things about Blind Frog Ranch is the **persistent and varied nature of the unexplained phenomena reported there**.  While many focus on the potential for Bigfoot sightings, the ranch has a history of reported paranormal activity, including strange lights, orbs, sounds (described as growls, howls, and other unsettling noises), and unusual energy readings.  The consistent reporting of *multiple* types of unexplained events, rather than just one singular phenomenon, makes it particularly intriguing and contributes to the ongoing mystery surrounding the location.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Tell me something interesting about Blindfrog Ranch\")\n",
    "print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validate the conversation state persists** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Your name is Rod.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"What is my name?\")\n",
    "print(f\"Response: {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Information about the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: models/chat-bison-001\n",
      "Model: models/text-bison-001\n",
      "Model: models/embedding-gecko-001\n",
      "Model: models/gemini-1.0-pro-latest\n",
      "Model: models/gemini-1.0-pro\n",
      "Model: models/gemini-pro\n",
      "Model: models/gemini-1.0-pro-001\n",
      "Model: models/gemini-1.0-pro-vision-latest\n",
      "Model: models/gemini-pro-vision\n",
      "Model: models/gemini-1.5-pro-latest\n",
      "Model: models/gemini-1.5-pro-001\n",
      "Model: models/gemini-1.5-pro-002\n",
      "Model: models/gemini-1.5-pro\n",
      "Model: models/gemini-1.5-pro-exp-0801\n",
      "Model: models/gemini-1.5-pro-exp-0827\n",
      "Model: models/gemini-1.5-flash-latest\n",
      "Model: models/gemini-1.5-flash-001\n",
      "Model: models/gemini-1.5-flash-001-tuning\n",
      "Model: models/gemini-1.5-flash\n",
      "Model: models/gemini-1.5-flash-exp-0827\n",
      "Model: models/gemini-1.5-flash-002\n",
      "Model: models/gemini-1.5-flash-8b\n",
      "Model: models/gemini-1.5-flash-8b-001\n",
      "Model: models/gemini-1.5-flash-8b-latest\n",
      "Model: models/gemini-1.5-flash-8b-exp-0827\n",
      "Model: models/gemini-1.5-flash-8b-exp-0924\n",
      "Model: models/gemini-2.0-flash-exp\n",
      "Model: models/gemini-exp-1206\n",
      "Model: models/gemini-exp-1121\n",
      "Model: models/gemini-exp-1114\n",
      "Model: models/learnlm-1.5-pro-experimental\n",
      "Model: models/embedding-001\n",
      "Model: models/text-embedding-004\n",
      "Model: models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "    print(f\"Model: {model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description=('Alias that points to the most recent stable version of Gemini 1.5 Flash, our '\n",
      "                   'fast and versatile multimodal model for scaling across diverse tasks.'),\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "# Get details of a specific model\n",
    "\n",
    "for model in genai.list_models():\n",
    "    if model.name == \"models/gemini-1.5-flash\":\n",
    "        print(model)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Generation Parameters\n",
    "- Limit the number of tokens sent back in the response using max_output_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Dawn of Quantum Computing: A Revolution in Computation\n",
      "\n",
      "Quantum computing represents a paradigm shift in computation, moving beyond the classical binary logic of bits to leverage the bizarre and counterintuitive principles of quantum mechanics.  This nascent field promises to revolutionize various scientific and technological domains, tackling problems currently intractable even for the most powerful supercomputers. While still in its early stages, the potential impact of quantum computing is immense, sparking both excitement and apprehension about its future implications.\n",
      "\n",
      "Classical computers process information using bits\n"
     ]
    }
   ],
   "source": [
    "shorten_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=100)\n",
    ")\n",
    "\n",
    "response = shorten_model.generate_content(\"Write a 1000 word essay on quantum computing\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "Bits are zero, bits are one, that's the classic way, you see,\n",
      "But quantum's got a different spin, a probability.\n",
      "Superposition's the key, a blurry kind of state,\n",
      "Both zero and one at once, it's really quite great!\n",
      "\n",
      "(Chorus)\n",
      "Quantum computing, oh so fast,\n",
      "Solving problems in the past,\n",
      "Entanglement's mystic dance,\n",
      "A computational chance!\n",
      "\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "response = shorten_model.generate_content(\"Write a short song about quantum computing\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "Temperature controls how random the token selection is.\n",
    "\n",
    "- **High temperature**: More randomness, more diverse results.\n",
    "- **Low temperature**: Less randomness, more predictable results.\n",
    "- **Temperature of 0**: No randomness, always selects the most probable token.\n",
    "\n",
    "Temperature doesn't guarantee randomness but can influence the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rmorrison/anaconda3/envs/ai_dev/bin/python\n",
      "Package                      Version\n",
      "---------------------------- -----------\n",
      "altair                       5.4.1\n",
      "annotated-types              0.7.0\n",
      "anyio                        4.6.2.post1\n",
      "appnope                      0.1.4\n",
      "asttokens                    2.4.1\n",
      "attrs                        24.2.0\n",
      "beautifulsoup4               4.12.3\n",
      "blinker                      1.9.0\n",
      "cachetools                   5.5.0\n",
      "certifi                      2024.8.30\n",
      "charset-normalizer           3.4.0\n",
      "click                        8.1.7\n",
      "comm                         0.2.2\n",
      "contourpy                    1.3.1\n",
      "cycler                       0.12.1\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "distro                       1.9.0\n",
      "exceptiongroup               1.2.2\n",
      "executing                    2.1.0\n",
      "finvizfinance                1.1.0\n",
      "fonttools                    4.55.0\n",
      "frozendict                   2.4.6\n",
      "gitdb                        4.0.11\n",
      "GitPython                    3.1.43\n",
      "google-ai-generativelanguage 0.6.10\n",
      "google-api-core              2.23.0\n",
      "google-api-python-client     2.154.0\n",
      "google-auth                  2.36.0\n",
      "google-auth-httplib2         0.2.0\n",
      "google-auth-oauthlib         1.2.1\n",
      "google-generativeai          0.8.3\n",
      "googleapis-common-protos     1.66.0\n",
      "grpcio                       1.68.1\n",
      "grpcio-status                1.68.1\n",
      "h11                          0.14.0\n",
      "html5lib                     1.1\n",
      "httpcore                     1.0.7\n",
      "httplib2                     0.22.0\n",
      "httpx                        0.27.2\n",
      "idna                         3.10\n",
      "importlib_metadata           8.5.0\n",
      "ipykernel                    6.29.5\n",
      "ipython                      8.29.0\n",
      "jedi                         0.19.2\n",
      "Jinja2                       3.1.4\n",
      "jiter                        0.8.0\n",
      "joblib                       1.4.2\n",
      "jsonschema                   4.23.0\n",
      "jsonschema-specifications    2024.10.1\n",
      "jupyter_client               8.6.3\n",
      "jupyter_core                 5.7.2\n",
      "kiwisolver                   1.4.7\n",
      "lxml                         5.3.0\n",
      "markdown-it-py               3.0.0\n",
      "MarkupSafe                   3.0.2\n",
      "matplotlib                   3.9.2\n",
      "matplotlib-inline            0.1.7\n",
      "mdurl                        0.1.2\n",
      "multitasking                 0.0.11\n",
      "narwhals                     1.14.1\n",
      "nest_asyncio                 1.6.0\n",
      "numpy                        2.1.3\n",
      "oauthlib                     3.2.2\n",
      "openai                       1.55.1\n",
      "packaging                    24.2\n",
      "pandas                       2.2.3\n",
      "parso                        0.8.4\n",
      "patsy                        1.0.1\n",
      "peewee                       3.17.8\n",
      "pendulum                     3.0.0\n",
      "pexpect                      4.9.0\n",
      "pickleshare                  0.7.5\n",
      "pillow                       11.0.0\n",
      "pip                          24.3.1\n",
      "platformdirs                 4.3.6\n",
      "plotly                       5.24.1\n",
      "prompt_toolkit               3.0.48\n",
      "proto-plus                   1.25.0\n",
      "protobuf                     5.28.3\n",
      "psutil                       5.9.0\n",
      "ptyprocess                   0.7.0\n",
      "pure_eval                    0.2.3\n",
      "pyarrow                      18.0.0\n",
      "pyasn1                       0.6.1\n",
      "pyasn1_modules               0.4.1\n",
      "pydantic                     2.10.2\n",
      "pydantic_core                2.27.1\n",
      "pydeck                       0.9.1\n",
      "Pygments                     2.18.0\n",
      "pyparsing                    3.2.0\n",
      "python-dateutil              2.9.0.post0\n",
      "pytz                         2024.2\n",
      "pyzmq                        25.1.2\n",
      "referencing                  0.35.1\n",
      "requests                     2.32.3\n",
      "requests-oauthlib            2.0.0\n",
      "rich                         13.9.4\n",
      "rpds-py                      0.21.0\n",
      "rsa                          4.9\n",
      "scikit-learn                 1.5.2\n",
      "scipy                        1.14.1\n",
      "seaborn                      0.13.2\n",
      "setuptools                   75.6.0\n",
      "six                          1.16.0\n",
      "smmap                        5.0.1\n",
      "sniffio                      1.3.1\n",
      "soupsieve                    2.6\n",
      "stack-data                   0.6.2\n",
      "statsmodels                  0.14.4\n",
      "streamlit                    1.40.1\n",
      "tenacity                     9.0.0\n",
      "threadpoolctl                3.5.0\n",
      "time-machine                 2.16.0\n",
      "toml                         0.10.2\n",
      "tornado                      6.4.1\n",
      "tqdm                         4.67.1\n",
      "traitlets                    5.14.3\n",
      "typing_extensions            4.12.2\n",
      "tzdata                       2024.2\n",
      "uritemplate                  4.1.1\n",
      "urllib3                      2.2.3\n",
      "wcwidth                      0.2.13\n",
      "webencodings                 0.5.1\n",
      "wheel                        0.45.0\n",
      "yfinance                     0.2.50\n",
      "zipp                         3.21.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High Temperature Model**\n",
    "- temperature = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mustang\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n",
      "Mustang\n",
      " -------------------------\n",
      "Mustang\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random car... (respond in a single word)',request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prius\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n",
      "Prius\n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random car... (respond in a single word)',request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
